# SAM4DBodyCapture v0.5.0 Design: Motion Disambiguation

**Document Version:** 1.0  
**Created:** 2025-12-29  
**Status:** Planning Complete, Ready for Implementation  
**Reference:** This document captures the full design discussion for motion disambiguation feature.

---

## Quick Context for New Sessions

If starting a new chat session, share this document and say:

> "Continue from v0.5.0 Motion Disambiguation design. Current status: [see STATUS section below]"

---

## STATUS (Update this as work progresses)

```
[x] Phase 2A: Motion Analyzer Node - COMPLETE
    - SAM4DMotionAnalyzer node created
    - Height estimation from mesh
    - User height override option
    - Pelvis/joint tracking (2D + 3D)
    - Foot contact detection
    - Motion vector debug overlay
    - SUBJECT_MOTION and SCALE_INFO outputs
    
[ ] Phase 2B: Camera Solver Node - NOT STARTED
[ ] Phase 2C: Motion Decoder Node - NOT STARTED
[ ] Phase 3: Skeleton Export (v0.6.0) - PLANNED
```

**Last Updated:** 2025-12-29  
**Current Version:** v0.5.0 (Phase 2A complete)  
**Next Version:** v0.5.0 (continuing with Phase 2B)

---

## 1. Problem Statement

When viewing motion in video, it could be caused by:
1. **Subject moving** (person walking/running)
2. **Camera moving** (pan, tilt, dolly, truck)
3. **Both moving** (most common in real footage)

Without separating these, the exported FBX has:
- Character "drifting" when camera pans (even if they stood still)
- Camera motion baked into character position
- No way to reframe the shot in post

---

## 2. Solution: 3-Node Pipeline

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    MOTION DISAMBIGUATION PIPELINE                        │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  [Video Frames]    [Masks (SAM3)]    [MESH_SEQUENCE from SAM3DBody]     │
│        │                 │                        │                      │
│        │                 │                        ▼                      │
│        │                 │           ┌─────────────────────────────┐    │
│        │                 │           │  NODE 1: MOTION ANALYZER    │    │
│        │                 │           │  ─────────────────────────  │    │
│        │                 │           │  • Height estimation        │    │
│        │                 │           │  • Pelvis/joint tracking    │    │
│        │                 │           │  • Foot contact detection   │    │
│        │                 │           │  • Motion vector overlay    │    │
│        │                 │           └──────────────┬──────────────┘    │
│        │                 │                          │                    │
│        │                 │                          ▼                    │
│        │                 │                   SUBJECT_MOTION              │
│        │                 │                   SCALE_INFO                  │
│        │                 │                          │                    │
│        ▼                 ▼                          │                    │
│  ┌────────────────────────────────────┐             │                    │
│  │  NODE 2: CAMERA SOLVER             │             │                    │
│  │  ─────────────────────             │             │                    │
│  │  Shot Type: [User Dropdown]        │             │                    │
│  │  • Static                          │             │                    │
│  │  • Nodal (Tripod)                  │             │                    │
│  │  • Dolly/Track                     │             │                    │
│  │  • Crane/Jib                       │             │                    │
│  │  • Drone/Steadicam                 │             │                    │
│  │  • Handheld                        │             │                    │
│  │  • Hybrid                          │             │                    │
│  │                                    │             │                    │
│  │  Quality: KLT → LightGlue → LoFTR  │             │                    │
│  │  Camera motion vector overlay      │             │                    │
│  └─────────────────┬──────────────────┘             │                    │
│                    │                                │                    │
│                    ▼                                │                    │
│             CAMERA_EXTRINSICS                       │                    │
│                    │                                │                    │
│                    └────────────────┬───────────────┘                    │
│                                     │                                    │
│                                     ▼                                    │
│                    ┌───────────────────────────────────┐                 │
│                    │  NODE 3: MOTION DECODER           │                 │
│                    │  ────────────────────             │                 │
│                    │  • Compare camera vs subject      │                 │
│                    │  • Subtract camera contribution   │                 │
│                    │  • Foot contact anchoring         │                 │
│                    │  • Separate temporal smoothing    │                 │
│                    └─────────────────┬─────────────────┘                 │
│                                      │                                   │
│                          ┌───────────┴───────────┐                      │
│                          ▼                       ▼                       │
│                   CAMERA_EXTRINSICS       WORLD_MOTION                  │
│                   (cleaned/smoothed)      (true subject pos)            │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 3. Motion Disambiguation Cases

| Case | Pelvis Motion | Background Motion | Height Change | Conclusion |
|------|---------------|-------------------|---------------|------------|
| **Subject Moving** | 20px | 0px | Stable | Subject walking/running |
| **Camera Panning** | 20px | 20px (same dir) | Stable | Camera rotating, subject still |
| **Subject Approaching** | Small | 0px | Growing | Subject moving toward camera |
| **Camera Dollying In** | Small | Parallax shift | Growing | Camera moving toward subject |
| **Combined** | Variable | Variable | Variable | Decompose and separate |

---

## 4. Node Specifications

### Node 1: SAM4DMotionAnalyzer

**Purpose:** Analyze subject motion from SAM3DBody mesh sequence outputs.

**Inputs:**
| Name | Type | Required | Description |
|------|------|----------|-------------|
| mesh_sequence | SAM4D_MESH_SEQUENCE | Yes | From SAM3DBody Batch Process |
| subject_height_m | FLOAT | No | User override (0 = auto ~1.70m) |
| reference_frame | INT | No | Frame for height estimation (default: 0) |
| show_debug | BOOLEAN | No | Generate debug overlay |

**Outputs:**
| Name | Type | Description |
|------|------|-------------|
| subject_motion | SUBJECT_MOTION | Per-frame position/velocity/foot contact |
| scale_info | SCALE_INFO | Height estimation and scale factor |
| debug_overlay | IMAGE | Motion vectors on video (optional) |
| debug_info | STRING | Text summary |

**SUBJECT_MOTION Structure:**
```python
{
    "pelvis_2d": np.ndarray,       # [N, 2] screen position
    "pelvis_3d": np.ndarray,       # [N, 3] world position (meters)
    "velocity_2d": np.ndarray,     # [N-1, 2] screen velocity
    "velocity_3d": np.ndarray,     # [N-1, 3] world velocity
    "apparent_height": np.ndarray, # [N] height in pixels
    "depth_estimate": np.ndarray,  # [N] depth in meters
    "foot_contact": list,          # [N] "both"/"left"/"right"/"none"
    "joint_positions": np.ndarray, # [N, J, 3] all joint positions
}
```

**SCALE_INFO Structure:**
```python
{
    "mesh_height": float,          # Raw mesh height
    "estimated_height": float,     # From joint chain
    "actual_height_m": float,      # Final height (user or estimated)
    "scale_factor": float,         # Multiply mesh units → meters
    "leg_length": float,           # Pelvis to ankle
    "torso_head_length": float,    # Pelvis to head
}
```

**Debug Overlay Shows:**
- Green dot: Pelvis position
- Yellow arrow: Velocity vector
- Foot contact state (color-coded text)
- Depth estimate (meters)
- Height reference

---

### Node 2: SAM4DCameraSolver

**Purpose:** Solve camera motion from video using background tracking.

**Inputs:**
| Name | Type | Required | Description |
|------|------|----------|-------------|
| images | IMAGE | Yes | Video frames |
| foreground_masks | MASK | Yes | Person masks (excluded from tracking) |
| shot_type | DROPDOWN | Yes | See shot types below |
| quality | DROPDOWN | No | "Fast" / "Balanced" / "Best" |
| camera_intrinsics | CAMERA_INTRINSICS | No | From MoGe2 |
| smoothing | INT | No | Temporal smoothing window |
| show_debug | BOOLEAN | No | Generate debug overlay |

**Shot Types:**
- Auto (Detect)
- Static
- Nodal (Tripod)
- Dolly/Track
- Crane/Jib
- Drone/Steadicam
- Handheld
- Hybrid

**Quality Modes:**
| Mode | Primary | Fallback | Speed | Accuracy |
|------|---------|----------|-------|----------|
| Fast | KLT | ORB | ⚡⚡⚡ | ⭐⭐ |
| Balanced | KLT | LightGlue | ⚡⚡ | ⭐⭐⭐ |
| Best | LoFTR | LightGlue | ⚡ | ⭐⭐⭐⭐ |

**Outputs:**
| Name | Type | Description |
|------|------|-------------|
| camera_motion | CAMERA_EXTRINSICS | Per-frame pan/tilt/roll/translation |
| debug_overlay | IMAGE | Camera motion vectors on video |
| status | STRING | Shot type detected, match count, etc. |

**Debug Overlay Shows:**
- Cyan dots: Tracked background features
- Magenta arrows: Feature flow vectors
- Yellow text: Pan/Tilt/Roll values
- Green text: Detected shot type

---

### Node 3: SAM4DMotionDecoder

**Purpose:** Separate camera and subject motion into clean outputs.

**Inputs:**
| Name | Type | Required | Description |
|------|------|----------|-------------|
| camera_motion | CAMERA_EXTRINSICS | Yes | From Camera Solver |
| subject_motion | SUBJECT_MOTION | Yes | From Motion Analyzer |
| scale_info | SCALE_INFO | No | For depth calculations |
| smoothing_camera | INT | No | Separate smoothing for camera |
| smoothing_subject | INT | No | Separate smoothing for subject |

**Outputs:**
| Name | Type | Description |
|------|------|-------------|
| camera_clean | CAMERA_EXTRINSICS | Cleaned/smoothed camera motion |
| world_motion | WORLD_MOTION | True subject world-space motion |
| debug_info | STRING | Disambiguation summary |

**WORLD_MOTION Structure:**
```python
{
    "positions": np.ndarray,       # [N, 3] world position in meters
    "velocities": np.ndarray,      # [N-1, 3] world velocity
    "rotations": np.ndarray,       # [N, 3] world rotation (if available)
    "confidence": np.ndarray,      # [N] per-frame confidence
    "grounded_frames": list,       # Frame indices with foot contact
}
```

---

## 5. Export Integration

Updated Export node with new motion inputs:

```python
class SAM4DExportCharacterFBX:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "mesh_sequence": ("SAM4D_MESH_SEQUENCE",),
                "filename": ("STRING", {"default": "character"}),
            },
            "optional": {
                # NEW: Motion inputs
                "camera_motion": ("CAMERA_EXTRINSICS",),
                "world_motion": ("WORLD_MOTION",),
                
                # Export mode
                "export_mode": ([
                    "Both Separated",       # Camera + subject both animated
                    "Camera Compensation",  # Static camera, motion on subject
                    "Subject Only",         # Only character, no camera
                    "Camera Only",          # Only camera, character at origin
                ], {"default": "Both Separated"}),
                
                # ... existing options ...
            }
        }
```

---

## 6. Dependencies

### Required (via pip)
```
kornia          # LoFTR matcher
scipy           # Smoothing/interpolation
```

### Optional (better quality)
```
LightGlue       # git clone https://github.com/cvg/LightGlue.git && pip install -e .
```

### install.py Update
```bash
python install.py --motion   # Install motion analysis dependencies
```

---

## 7. Future: Skeleton Export (v0.6.0)

**Problem:** SAM3DBody outputs joint positions, but FBX skeletons need rotations.

**Solution:** Convert translation to hierarchical rotation:

```
For each bone (child):
  1. Get parent position
  2. Get child position  
  3. Calculate bone direction: dir = child - parent
  4. Get reference direction (T-pose)
  5. Calculate rotation: rot = rotation_between(ref, dir)
  6. Convert to local space (relative to parent)
  7. Store as quaternion
```

**Benefits:**
- Bone lengths constant (no stretching)
- Subject height preserved
- Compatible with retargeting (Maya HIK, Mixamo)
- Smaller file size
- Better interpolation

---

## 8. Implementation Order

### Phase 2A: Motion Analyzer (First Priority)
1. Create `nodes/motion_analyzer.py`
2. Height estimation from mesh
3. Pelvis tracking (2D + 3D)
4. Foot contact detection
5. Debug overlay generation
6. Output SUBJECT_MOTION and SCALE_INFO types

### Phase 2B: Camera Solver
1. Create `nodes/camera_solver.py`
2. KLT optical flow (always available)
3. LightGlue integration (optional)
4. LoFTR fallback via kornia (optional)
5. Shot type handling
6. Debug overlay generation

### Phase 2C: Motion Decoder
1. Create `nodes/motion_decoder.py`
2. Motion comparison logic
3. Camera contribution subtraction
4. Foot contact anchoring
5. Separate temporal smoothing

### Phase 2D: Export Integration
1. Add new inputs to Export node
2. New export modes
3. Update Blender script

---

## 9. Key Decisions Made

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Node granularity | 3 separate nodes | Easier debugging |
| Node naming | Analyzer → Solver → Decoder | Clear, pronounceable |
| LightGlue install | Optional via git clone | Not pip-installable |
| LoFTR source | kornia package | Already pip-installable |
| Scale estimation | Auto with user override | Flexibility |
| Debug visualization | Motion vectors as overlay | Visual debugging |
| Shot type | User dropdown | Avoid auto-detect complexity initially |
| Default shot type | TBD for long videos | Future optimization |

---

## 10. Files to Create

```
ComfyUI-SAM4DBodyCapture/
├── nodes/
│   ├── motion_analyzer.py      # NEW: Phase 2A
│   ├── camera_solver.py        # NEW: Phase 2B
│   ├── motion_decoder.py       # NEW: Phase 2C
│   └── export_nodes.py         # UPDATE: Add motion inputs
├── docs/
│   └── v0.5.0_motion_disambiguation_design.md  # THIS FILE
└── install.py                  # UPDATE: Add --motion option
```

---

## 11. References

### SAM3DBody2abc Camera Solver
Location: `/home/claude/temp_sam3d2abc/ComfyUI-SAM3DBody2abc/nodes/camera_solver.py`

Key functions to adapt:
- `run_klt()` - KLT optical flow
- `run_lightglue()` - LightGlue matching
- `run_loftr()` - LoFTR matching
- `solve_rotation_only()` - Homography decomposition
- `rotation_matrix_to_euler()` - R → pan/tilt/roll

### SMPL Joint Indices
```python
PELVIS = 0
LEFT_HIP = 1
RIGHT_HIP = 2
SPINE1 = 3
LEFT_KNEE = 4
RIGHT_KNEE = 5
SPINE2 = 6
LEFT_ANKLE = 7
RIGHT_ANKLE = 8
SPINE3 = 9
LEFT_FOOT = 10
RIGHT_FOOT = 11
NECK = 12
LEFT_COLLAR = 13
RIGHT_COLLAR = 14
HEAD = 15
LEFT_SHOULDER = 16
RIGHT_SHOULDER = 17
LEFT_ELBOW = 18
RIGHT_ELBOW = 19
LEFT_WRIST = 20
RIGHT_WRIST = 21
```

---

## 12. Session Continuity Checklist

When starting a new session, verify:

- [ ] This design document is accessible
- [ ] Current STATUS section is accurate
- [ ] Any code changes since last session are noted
- [ ] v0.4.2 package is the baseline

**Quick start for new session:**
```
"I'm continuing work on SAM4DBodyCapture v0.5.0 Motion Disambiguation.
The design doc is at: docs/v0.5.0_motion_disambiguation_design.md
Current status: [Phase 2A/2B/2C] [NOT STARTED/IN PROGRESS/COMPLETE]
Let's continue with [specific task]."
```

---

*End of Design Document*
